### Set global variables


SEED = 2718

# fragment variables
LENGTH = list(range(1, 250+1)) + [1000]
NUMFRAGS = 1000000
TOOLNAMES = [
    "leeHom", "AdapterRemoval", "ClipAndMerge", "seqtk_adna_trim", 
    "bbmerge", "fastp", "SeqPrep"
    ]

# project directory
PROJECTDIR = "/home/projects2/DNA_reconstruct/merging_insert_lengths_publication"

# output directories
OUTDIR = PROJECTDIR + "/output"
OUTDIR_BEN = OUTDIR + "/benchmarks"
OUTDIR_SIM = OUTDIR + "/simulations"
OUTDIR_REC = OUTDIR + "/reconstructions"
OUTDIR_EVA = OUTDIR + "/evaluation"
OUTDIR_PLOT = OUTDIR + "/plots"

# tools
FRAGSIM = "/home/ctools/gargammel/src/fragSim"
ADPTSIM = "/home/ctools/gargammel/src/adptSim"
ART = "/home/ctools/gargammel/art_src_MountRainier_Linux/art_illumina"
LEEHOM = "/home/projects/gabriel/leehom_interweaved/leeHom" 
ADPTREM = "/home/ctools/adapterremoval-2.3.2/build/AdapterRemoval"
CLIPMERGE = "/home/ctools/ClipAndMerge-1.7.8/build/libs/ClipAndMerge-1.7.8.jar"
SEQTK = "/home/ctools/seqtk-1.3/seqtk"
ADNA = "/home/ctools/adna/adna-trim"
BBMERGE = "/home/ctools/bbmap_38_91/bbmerge.sh"
FASTP = "/home/ctools/fastp/fastp"
SEQPREP = "/home/ctools/SeqPrep-1.3.2/SeqPrep"

# script
EVAL_SCRIPT = PROJECTDIR + "/evaluate.py"
MERGE_SCRIPT = PROJECTDIR + "/merge_csv.sh"
PLOT_SCRIPT = PROJECTDIR + "/plot.py"

# Input genome and adapter sequences
IN_FILE = "/home/databases/genomes/Homo_sapiens/CHM13_T2T/CHM13_T2T.fa"
ADPT1 = "AGATCGGAAGAGCACACGTCTGAACTCCAGTCACCGATTCGATCTCGTATGCCGTCTTCTGCTTG"
ADPT2 = "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATTT"


### Run all


rule all:
    input:
        OUTDIR_PLOT + "/final/merging_insert_lengths.png",
    run:
        #shell("rm {OUTDIR_SIM}/*_adpt.fa")
        #shell("rm {OUTDIR_SIM}/*_s1.fq")
        #shell("rm {OUTDIR_SIM}/*_s2.fq")
        shell("gzip {OUTDIR_SIM}/*")


### Simulation


rule simulate_fragments:
    """
    gargammel fragSim:
    simulation of ancient DNA fragments being retrieved at random from 
    the genome.
    """
    input:
        IN_FILE,
    output:
        OUTDIR_SIM + "/gen_n{n}_l{l}_frag.fa",
    benchmark:
        OUTDIR_BEN + "/simulate_fragments/gen_n{n}_l{l}_frag.tsv"
    wildcard_constraints:
        n="\d+",
        l="\d+",
    shell:
        (
            "{FRAGSIM}"
            " -n {wildcards.n}"
            " -l {wildcards.l}"
            " {input}"
            " > {output}"
            )


rule add_adapters:
    """
    gargammel adptSim:
    adding of adapters to create raw Illumina reads (without errors and
    quality scores)

    Output reads as ART (unzipped fasta) with wrap-around for paired-end
    mode.
    """
    input:
        OUTDIR_SIM + "/gen_n{n}_l{l}_frag.fa",
    output:
        OUTDIR_SIM + "/gen_n{n}_l{l}_adpt.fa",
    benchmark:
        OUTDIR_BEN + "/add_adapters/gen_n{n}_l{l}_adpt.tsv"
    wildcard_constraints:
        n="\d+",
        l="\d+",
    shell:
        # -l        : Desired read length
        # -artp     : Output reads as ART with wrap-around
        (
            "{ADPTSIM}" 
            " -l 125"
            " -artp" 
            " {output}"
            " {input}"
        )


rule simulate_reads:
    """
    add sequencing errors and corresponding quality scores
    Illumina HiSeq 2500 (125bp, 150bp)
    """
    input:
        OUTDIR_SIM + "/gen_n{n}_l{l}_adpt.fa",
    output:
        OUTDIR_SIM + "/gen_n{n}_l{l}_s1.fq",
        OUTDIR_SIM + "/gen_n{n}_l{l}_s2.fq",
    params:
        out_prefix=OUTDIR_SIM + "/gen_n{n}_l{l}_s",
    benchmark:
        OUTDIR_BEN + "/simulate_reads/gen_n{n}_l{l}_reads.tsv"
    wildcard_constraints:
        n="\d+",
        l="\d+",
    run:
        # --insRate     : insertion rate
        # -dr           : deletion rate
        # --seqSys      : HS25: Illumina HiSeq 2500 (125bp, 150bp)
        # --len         : read length
        # --rcount      : number of read pairs to be generated per sequence
        # --paired      : paired-end read simulation
        # --amplicon    : amplicon sequencing simulation
        # --noALN       : do not output ALN alignment file
        # --quiet       : turn off end of run summary
        # --rndSeed     : the seed for random number generator, use a
        #                 fixed seed to generate two identical datasets
        #                 from different runs
        shell(
            "{ART}"
            " --insRate 0"
            " --insRate2 0"
            " -dr 0"
            " -dr2 0"
            " --seqSys HS25"
            " --len 125"
            " --rcount 1"
            " --paired"
            " --amplicon"
            " --noALN"
            " --quiet"
            " -i {input}"
            " -o {params.out_prefix}"
            " --rndSeed {SEED}"
        )


### Reconstruction


rule leeHom:
    """Reconstruction using leeHom"""
    input:
        s1=OUTDIR_SIM + "/gen_n{n}_l{l}_s1.fq.gz",
        s2=OUTDIR_SIM + "/gen_n{n}_l{l}_s2.fq.gz",
    output:
        OUTDIR_REC + "/leeHom/gen_n{n}_l{l}_merged.fq.gz",
    wildcard_constraints:
        n="\d+",
        l="\d+",
    params:
        out_prefix=OUTDIR_REC + "/leeHom/gen_n{n}_l{l}_merged",
        rm1=OUTDIR_REC + "/leeHom/gen_n{n}_l{l}_merged_r1.fq.gz",
        rm2=OUTDIR_REC + "/leeHom/gen_n{n}_l{l}_merged_r2.fq.gz",
        rm3=OUTDIR_REC + "/leeHom/gen_n{n}_l{l}_merged.fail.fq.gz",
        rm4=OUTDIR_REC + "/leeHom/gen_n{n}_l{l}_merged_r1.fail.fq.gz",
        rm5=OUTDIR_REC + "/leeHom/gen_n{n}_l{l}_merged_r2.fail.fq.gz",
    benchmark:
        OUTDIR_BEN + "/leeHom/gen_n{n}_l{l}_merged.tsv"
    run:
        # (default : PHRED33)
        # -t [threads]            Use multiple cores (default : 1)
        shell(
            "{LEEHOM}"
            " --ancientdna"
            " --adapterFirstRead {ADPT1}"
            " --adapterSecondRead {ADPT2}"
            " -fq1 {input.s1}"
            " -fq2 {input.s2}"
            " -fqo {params.out_prefix}"
        )
        shell(
            "rm"
            " {params.rm1}"
            " {params.rm2}"
            " {params.rm3}"
            " {params.rm4}"
            " {params.rm5}"
        )


rule AdapterRemoval:
    """Reconstruction using AdapterRemoval"""
    input:
        s1=OUTDIR_SIM + "/gen_n{n}_l{l}_s1.fq.gz",
        s2=OUTDIR_SIM + "/gen_n{n}_l{l}_s2.fq.gz",
    output:
        OUTDIR_REC + "/AdapterRemoval/gen_n{n}_l{l}_merged.fq.gz",
    wildcard_constraints:
        n="\d+",
        l="\d+",
    params:
        merged=OUTDIR_REC + "/AdapterRemoval/gen_n{n}_l{l}_merged.fq.gz",
        basename=OUTDIR_REC + "/AdapterRemoval/gen_n{n}_l{l}_unmerged",
    benchmark:
        OUTDIR_BEN + "/AdapterRemoval/gen_n{n}_l{l}_merged.tsv"
    run:
        # --seed SEED
        #     Sets the RNG seed used when choosing between bases with 
        #     equal Phred scores.
        # --qualitymax BASE
        #     Specifies the maximum Phred score expected in input files, and used
        #     when writing output. ASCII encoded values are limited to the
        #     characters '!' (ASCII = 33) to '~' (ASCII = 126), meaning that
        #     possible scores are 0 - 93 with offset 33, and 0 - 62 for offset 64
        #     and Solexa scores [default: 41].
        #
        # number of threads is by default 1
        shell(
            "{ADPTREM}"
            " --gzip"
            " --collapse"
            " --adapter1 {ADPT1}"
            " --adapter2 {ADPT2}"
            " --file1 {input.s1}"
            " --file2 {input.s2}"
            " --basename {params.basename}"
            " --outputcollapsed {params.merged}"
            " --seed {SEED}"
        )
        shell(
            "rm {params.basename}*"
        )
        

rule ClipAndMerge:
    """Reconstruction using ClipAndMerge"""
    input:
        s1=OUTDIR_SIM + "/gen_n{n}_l{l}_s1.fq.gz",
        s2=OUTDIR_SIM + "/gen_n{n}_l{l}_s2.fq.gz",
    output:
        m=OUTDIR_REC + "/ClipAndMerge/gen_n{n}_l{l}_merged.fq.gz",
    wildcard_constraints:
        n="\d+",
        l="\d+",
    benchmark:
        OUTDIR_BEN + "/ClipAndMerge/gen_n{n}_l{l}_merged.tsv"
    shell:
        # -l INTEGER      : Discard sequences shorter than 
        #                   this number of nucleotides after adapter 
        #                   clipping. (default: 25)
        # -p INTEGER      : Minimal number of nucleotides that have to
        #                   overlap in order to merge the forward and
        #                   reverse read. (default: 10)
        # -u FORWARD_FILE REVERSE_FILE : 
        #                   Write unmerged forward and 
        #                   reverse reads to extra files. the regular
        #                   output file then only contains merged reads!
        (
            "java -jar {CLIPMERGE}"
            " -in1 {input.s1}"
            " -in2 {input.s2}"
            " -f {ADPT1}"
            " -r {ADPT2}"
            " -o {output.m}"
            " -u /dev/null /dev/null"
        )


rule seqtk_adna_trim:
    """Reconstruction using seqtk and adna-trim"""
    input:
        s1=OUTDIR_SIM + "/gen_n{n}_l{l}_s1.fq.gz",
        s2=OUTDIR_SIM + "/gen_n{n}_l{l}_s2.fq.gz",
    output:
        OUTDIR_REC + "/seqtk_adna_trim/gen_n{n}_l{l}_merged.fq.gz",
    wildcard_constraints:
        n="\d+",
        l="\d+",
    benchmark:
        OUTDIR_BEN + "/seqtk_adna_trim/gen_n{n}_l{l}_merged.tsv"
    shell:
        # seqtk mergepe: interleave two paired-end FASTA/Q files
        # adna-trim:
        # -l INT       min read/fragment length to output [default: 30]
        # -t INT       number of threads [default: 2]
        # -o INT       min overlap length [10]
        # -p STR       output PE reads to STR.R[12].fq.gz 
        #              [default: discard pe]
        (
            "{SEQTK} mergepe"
            " {input.s1}"
            " {input.s2} |"
            " {ADNA}"
            " -t 1"
            " -"
            " | gzip "
            " > {output}"
        )


rule bbmerge:
    """Reconstruction using BBMerge"""
    input:
        s1=OUTDIR_SIM + "/gen_n{n}_l{l}_s1.fq.gz",
        s2=OUTDIR_SIM + "/gen_n{n}_l{l}_s2.fq.gz",
    output:
        m=OUTDIR_REC + "/bbmerge/gen_n{n}_l{l}_merged.fq.gz",
    wildcard_constraints:
        n="\d+",
        l="\d+",
    benchmark:
        OUTDIR_BEN + "/bbmerge/gen_n{n}_l{l}_merged.tsv"
    shell:
        # t=1             Set threads to 1
        # mininsert=35    Minimum insert size to merge reads.
        # mininsert0=35   Insert sizes less than this will not be considered.
        #                 Must be less than or equal to mininsert.
        # minoverlap=12   Minimum number of overlapping bases to allow merging.
        # minoverlap0=8   Overlaps shorter than this will not be considered.
        #                 Must be less than or equal to minoverlap.
        # outu=<file>   : File for unmerged reads.
        (
            "{BBMERGE}"
            " in1={input.s1}"
            " in2={input.s2}"
            " out={output.m}"
            " adapter1={ADPT1}"
            " adapter2={ADPT2}"
            " t=1"
        )


rule fastp:
    """Reconstruction using fastp"""
    input:
        s1=OUTDIR_SIM + "/gen_n{n}_l{l}_s1.fq.gz",
        s2=OUTDIR_SIM + "/gen_n{n}_l{l}_s2.fq.gz",
    output:
        m=OUTDIR_REC + "/fastp/gen_n{n}_l{l}_merged.fq.gz",
    wildcard_constraints:
        n="\d+",
        l="\d+",
    benchmark:
        OUTDIR_BEN + "/fastp/gen_n{n}_l{l}_fp.tsv"
    shell:
        # --overlap_len_require            
        #           the minimum length to detect overlapped region of PE
        #           reads. This will affect overlap analysis based PE
        #           merge, adapter trimming and correction. 30 by
        #           default. (int [=30])
        #   -L, --disable_length_filtering       
        #           length filtering is enabled by default. If this
        #           option is specified, length filtering is disabled
        #   -l, --length_required                
        #           reads shorter than length_required will be 
        #           discarded, default is 15. (int [=15])    
        # --overlap_len_require            
        #           the minimum length to detect overlapped region of PE
        #           reads. This will affect overlap analysis based PE 
        #           merge, adapter trimming and correction. 30 by
        #           default. (int [=30])    
        (
            "{FASTP}"
            " --merge "
            " --merged_out {output.m}"
            " --in1 {input.s1}"
            " --in2 {input.s2}"
            " --adapter_sequence {ADPT1}"
            " --adapter_sequence_r2 {ADPT2}"
            " --json /dev/null"
            " --html /dev/null"
        )


rule SeqPrep:
    """Reconstruction using SeqPrep"""
    input:
        s1=OUTDIR_SIM + "/gen_n{n}_l{l}_s1.fq.gz",
        s2=OUTDIR_SIM + "/gen_n{n}_l{l}_s2.fq.gz",
    output:
        m=OUTDIR_REC + "/SeqPrep/gen_n{n}_l{l}_merged.fq.gz",
    wildcard_constraints:
        n="\d+",
        l="\d+",
    benchmark:
        OUTDIR_BEN + "/SeqPrep/gen_n{n}_l{l}_merged.tsv"
    shell:
        # The output is always gziped compressed.
        # -f <first read input fastq filename>
        # -r <second read input fastq filename>
        # -s <perform merging and output the merged reads to this file>
        # -1 <first read output fastq filename>
        # -2 <second read output fastq filename>
        # -L <Minimum length of a trimmed/merged read; default = 30>
        # -A <forward read primer/adapter sequence to trim>
        # -B <reverse read primer/adapter sequence to trim>
        (
            "{SEQPREP}"
            " -f {input.s1}"
            " -r {input.s2}"
            " -s {output.m}"
            " -1 /dev/null"
            " -2 /dev/null"
            " -A {ADPT1}"
            " -B {ADPT2}"
        )


### Evaluate trimming performance


rule evaluate:
    "Run the evaluation script"
    resources:
        mem_mb = 10000
    input:
        orig=OUTDIR_SIM + "/gen_n{n}_l{l}_frag.fa.gz", 
        rec=OUTDIR_REC + "/{tool_name}/gen_n{n}_l{l}_merged.fq.gz",
    output:
        OUTDIR_EVA + "/{tool_name}/gen_n{n}_l{l}.csv"
    conda:
        PROJECTDIR + "/environment.yaml"
    run:
        shell(
            "python3 {EVAL_SCRIPT}"
            " --out {output}"
            " --nfrags {wildcards.n}"
            " --fraglen {wildcards.l}"
            " --tool {wildcards.tool_name}"
            " --templates {input.orig}"
            " --mreads {input.rec}"
        )


rule merge_csv:
    input:
        expand(
            OUTDIR_EVA + "/{tool_name}/gen_n{n}_l{l}.csv",
            tool_name=TOOLNAMES,
            n=NUMFRAGS,
            l=LENGTH,
        ),
    output:  
        OUTDIR_EVA + "/all_merged.csv",
    run:
        shell("{MERGE_SCRIPT}")


rule plot:
    input:
        OUTDIR_EVA + "/all_merged.csv"
    output:
        expand(
            OUTDIR_PLOT + "/edit_distances_{tool_name}.png",
            tool_name=TOOLNAMES,
        ),
    conda:
        PROJECTDIR + "/environment.yaml"
    shell:
        (
            "python3 {PLOT_SCRIPT}"
            " -i {input}"
            " -o {OUTDIR_PLOT}"
        )


rule create_final_plots:
    input:
        PROJECTDIR + "/create_final_plots.sh",
        expand(
            OUTDIR_PLOT + "/edit_distances_{tool_name}.png",
            tool_name=TOOLNAMES,
        ),
    output:
        OUTDIR_PLOT + "/final/merging_insert_lengths.png"
    shell:
        ("bash create_final_plots.sh")
